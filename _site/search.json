[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "1 Intro reading\nThis is the first chapter of my lab journal focusing on the Tidyverse.\nAt first i started to read the chapter Intro to the Tidyverse and used the INSTALL_PKGS.R file to install all the required packages.\nAfter reading through the basics like Pipes, Tibbles and Import.\n\ntibble(\n    x = 1:50,\n    y = runif(50), \n    z = x + y^2,\n    outcome = rnorm(50)\n  )\n\n\n\n  \n\n\n\n\nlibrary(readr)\ndataset_tbl <- read_csv(\"../../src/test-data.csv\",show_col_types = FALSE)\nreadr::problems(dataset_tbl)\n\n\n\n  \n\n\n\n\n2 Tidy Exercise\n\nTable 1 is tidy\nTable 2 is not tidy, since the name of the variable is stored in every row of the table and not just as a column name with the values in that column.\nTable 3 is untidy, since here the variable names are addressing individual rows whereas they should do that with columns. So the table would need to be rotated.\nTable 4 is untidy, since multiple variables are stored per column.\n\n3 Diamonds\n\ndiamonds2 <- readRDS(\"../../src/diamonds2.rds\")\ndiamonds2\n\n\n\n  \n\n\ndiamonds2_long <- diamonds2 %>% \n  pivot_longer(cols      = c(\"2008\", \"2009\"), \n               names_to  = 'year', \n               values_to = 'price') %>% \n  head(n = 5)\ndiamonds2_long\n\n\n\n  \n\n\nlm(price ~ ., data = diamonds2_long)\n\n#> \n#> Call:\n#> lm(formula = price ~ ., data = diamonds2_long)\n#> \n#> Coefficients:\n#> (Intercept)     cutIdeal   cutPremium     year2009  \n#>         237           89           89            6\n\ndiamonds3 <- readRDS(\"../../src/diamonds3.rds\")\ndiamonds3 %>% pivot_wider(names_from = \"dimension\",values_from = \"measurement\") %>% head(n=5)\n\n\n\n  \n\n\ndiamonds4 <- readRDS(\"../../src/diamonds4.rds\")\ndiamonds4 %>% separate(col = dim, into = c(\"x\",\"y\",\"z\"),sep = \"/\", convert = T)\n\n\n\n  \n\n\ndiamonds5 <- readRDS(\"../../src/diamonds5.rds\")\ndiamonds5 %>% unite(clarity,clarity_prefix,clarity_suffix,sep='')\n\n\n\n  \n\n\n\n\n4 Transform\n\nlibrary(ggplot2) # To load the diamonds dataset\nlibrary(dplyr)\ndiamonds %>% \n    filter(cut == 'Premium', carat >= 0.3) %>% \n    head(5)\n\n\n\n  \n\n\ndiamonds %>% \n  select(x:z, everything()) %>%\n  rename(colorcode = color) %>%\n  head(n = 5)\n\n\n\n  \n\n\ndiamonds %>%\n  group_by(color) %>%\n  summarize(max_price  = max(price),\n            mean_price = mean(price),\n            min_price  = min(price))\n\n\n\n  \n\n\nlibrary(lubridate)\nymd(19690716)\n\n#> [1] \"1969-07-16\"\n\n\n\n5 Business case\nTried out all the examples. See SALES_ANALYSIS.R in the src directory for details.\n\n6 Challenge\nWorked pretty well adjusting the existing plots for year and category to location and year.\nCode is also present in SALES_ANALYSIS.R file.\n\n# 6.3 Sales by location\n\n# Step 1 - Manipulate\nsales_by_location_tbl <- bike_orderlines_wrangled_tbl %>%\n  # Seperate city and state into two separate columns\n  separate(col = location,\n           into = c(\"city\",\"state\"),\n           sep = \", \",\n           convert = T) %>%\n  \n  # Select columns\n  select(state, total_price) %>%\n  \n  # Grouping by year and summarizing sales\n  group_by(state) %>% \n  summarize(sales = sum(total_price)) %>%\n  \n  # Optional: Add a column that turns the numbers into a currency format \n  # (makes it in the plot optically more appealing)\n  # mutate(sales_text = scales::dollar(sales)) <- Works for dollar values\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n \n# Step 2 - Visualize\nsales_by_location_tbl %>%\n  \n  # Setup canvas with the columns year (x-axis) and sales (y-axis)\n  ggplot(aes(x = state, y = sales)) +\n  \n  # Geometries\n  geom_col(fill = \"#2DC6D6\") + # Use geom_col for a bar plot\n  geom_label(aes(label = sales_text)) + # Adding labels to the bars\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n  \n  # Formatting\n  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. \n  # Again, we have to adjust it for euro values\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title    = \"Revenue by location\",\n    subtitle = \"Upward Trend\",\n    x = \"\", # Override defaults for x and y\n    y = \"Revenue\"\n  ) +\n\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n#> `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# 6.4 Sales by location and year\n# Step 1 - Manipulate\nsales_by_location_and_year_tbl <- bike_orderlines_wrangled_tbl %>%\n  # Seperate city and state into two separate columns\n  separate(col = location,\n           into = c(\"city\",\"state\"),\n           sep = \", \",\n           convert = T) %>%\n  \n  # Select columns and add a year\n  select(state, total_price, order_date) %>%\n  mutate(year = year(order_date)) %>%\n  \n  # Group by and summarize year and main catgegory\n  group_by(state, year) %>%\n  summarise(sales = sum(total_price)) %>%\n  ungroup() %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n#> `summarise()` has grouped output by 'state'. You can override using the\n#> `.groups` argument.\n\n# Step 2 - Visualize\nsales_by_location_and_year_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = year, y = sales, fill = year)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Facet\n  facet_wrap(~ state) +\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by location and year\",\n    subtitle = \"Each year has an upward trend\",\n    fill = \"Year\" # Changes the legend name\n  ) +\n\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "1 CHALLENGE\nAsking the API for the weather on Gran Canaria in this week.\n\n# Task 1. Asking an API.\nlibrary(ggplot2)\n\nweather_api <- function() {\n  url <- modify_url(url = \"https://api.open-meteo.com/v1/forecast?latitude=27.7564556&longitude=-15.5910148&current_weather=true&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\")\n  resp <- GET(url)\n  stop_for_status(resp) # automatically throws an error if a request did not succeed\n}\n\nweather <- weather_api() %>% content()\n\nweather_tbl <- tibble(time = weather[[\"hourly\"]][[\"time\"]],temp = weather[[\"hourly\"]][[\"temperature_2m\"]])\nweather_df <- weather_tbl %>% data.frame\nprint(weather_df) # Print weather as df\n\n#>                 time temp\n#> 1   2023-05-02T00:00 23.8\n#> 2   2023-05-02T01:00 22.7\n#> 3   2023-05-02T02:00 22.3\n#> 4   2023-05-02T03:00 21.8\n#> 5   2023-05-02T04:00 21.4\n#> 6   2023-05-02T05:00 20.9\n#> 7   2023-05-02T06:00 20.4\n#> 8   2023-05-02T07:00 20.4\n#> 9   2023-05-02T08:00 22.2\n#> 10  2023-05-02T09:00 23.8\n#> 11  2023-05-02T10:00 25.4\n#> 12  2023-05-02T11:00 26.5\n#> 13  2023-05-02T12:00 26.9\n#> 14  2023-05-02T13:00 26.7\n#> 15  2023-05-02T14:00 26.2\n#> 16  2023-05-02T15:00 27.5\n#> 17  2023-05-02T16:00 29.2\n#> 18  2023-05-02T17:00 29.1\n#> 19  2023-05-02T18:00 26.7\n#> 20  2023-05-02T19:00 24.9\n#> 21  2023-05-02T20:00 23.9\n#> 22  2023-05-02T21:00 22.8\n#> 23  2023-05-02T22:00 22.1\n#> 24  2023-05-02T23:00 21.7\n#> 25  2023-05-03T00:00 21.3\n#> 26  2023-05-03T01:00 21.2\n#> 27  2023-05-03T02:00 21.2\n#> 28  2023-05-03T03:00 21.5\n#> 29  2023-05-03T04:00 21.2\n#> 30  2023-05-03T05:00 20.8\n#> 31  2023-05-03T06:00   20\n#> 32  2023-05-03T07:00 20.3\n#> 33  2023-05-03T08:00 21.3\n#> 34  2023-05-03T09:00 22.2\n#> 35  2023-05-03T10:00 24.5\n#> 36  2023-05-03T11:00 25.5\n#> 37  2023-05-03T12:00   26\n#> 38  2023-05-03T13:00 25.8\n#> 39  2023-05-03T14:00 25.8\n#> 40  2023-05-03T15:00 26.9\n#> 41  2023-05-03T16:00 27.3\n#> 42  2023-05-03T17:00 26.9\n#> 43  2023-05-03T18:00 25.9\n#> 44  2023-05-03T19:00 24.5\n#> 45  2023-05-03T20:00 23.6\n#> 46  2023-05-03T21:00 23.1\n#> 47  2023-05-03T22:00 22.6\n#> 48  2023-05-03T23:00 22.1\n#> 49  2023-05-04T00:00 21.8\n#> 50  2023-05-04T01:00 21.4\n#> 51  2023-05-04T02:00 21.1\n#> 52  2023-05-04T03:00 20.7\n#> 53  2023-05-04T04:00 20.5\n#> 54  2023-05-04T05:00 20.3\n#> 55  2023-05-04T06:00   20\n#> 56  2023-05-04T07:00 19.8\n#> 57  2023-05-04T08:00 20.5\n#> 58  2023-05-04T09:00 21.9\n#> 59  2023-05-04T10:00 23.4\n#> 60  2023-05-04T11:00 24.8\n#> 61  2023-05-04T12:00 25.9\n#> 62  2023-05-04T13:00 26.7\n#> 63  2023-05-04T14:00 27.2\n#> 64  2023-05-04T15:00 27.1\n#> 65  2023-05-04T16:00 26.6\n#> 66  2023-05-04T17:00 25.7\n#> 67  2023-05-04T18:00 24.4\n#> 68  2023-05-04T19:00 22.8\n#> 69  2023-05-04T20:00 21.2\n#> 70  2023-05-04T21:00 20.2\n#> 71  2023-05-04T22:00 19.8\n#> 72  2023-05-04T23:00 19.6\n#> 73  2023-05-05T00:00 19.5\n#> 74  2023-05-05T01:00 19.3\n#> 75  2023-05-05T02:00 19.2\n#> 76  2023-05-05T03:00 19.1\n#> 77  2023-05-05T04:00 19.1\n#> 78  2023-05-05T05:00 19.1\n#> 79  2023-05-05T06:00   19\n#> 80  2023-05-05T07:00   19\n#> 81  2023-05-05T08:00 19.9\n#> 82  2023-05-05T09:00 21.5\n#> 83  2023-05-05T10:00 22.9\n#> 84  2023-05-05T11:00   24\n#> 85  2023-05-05T12:00   25\n#> 86  2023-05-05T13:00 25.6\n#> 87  2023-05-05T14:00 25.9\n#> 88  2023-05-05T15:00 26.1\n#> 89  2023-05-05T16:00 26.1\n#> 90  2023-05-05T17:00 25.5\n#> 91  2023-05-05T18:00 24.3\n#> 92  2023-05-05T19:00   23\n#> 93  2023-05-05T20:00 21.4\n#> 94  2023-05-05T21:00 20.4\n#> 95  2023-05-05T22:00 19.8\n#> 96  2023-05-05T23:00 19.4\n#> 97  2023-05-06T00:00 19.2\n#> 98  2023-05-06T01:00   19\n#> 99  2023-05-06T02:00   19\n#> 100 2023-05-06T03:00   19\n#> 101 2023-05-06T04:00   19\n#> 102 2023-05-06T05:00 18.9\n#> 103 2023-05-06T06:00 18.9\n#> 104 2023-05-06T07:00 21.3\n#> 105 2023-05-06T08:00 22.3\n#> 106 2023-05-06T09:00 23.8\n#> 107 2023-05-06T10:00 24.9\n#> 108 2023-05-06T11:00   26\n#> 109 2023-05-06T12:00 27.1\n#> 110 2023-05-06T13:00 27.5\n#> 111 2023-05-06T14:00 27.7\n#> 112 2023-05-06T15:00 27.6\n#> 113 2023-05-06T16:00   27\n#> 114 2023-05-06T17:00 26.1\n#> 115 2023-05-06T18:00 24.9\n#> 116 2023-05-06T19:00 24.1\n#> 117 2023-05-06T20:00 23.4\n#> 118 2023-05-06T21:00 22.7\n#> 119 2023-05-06T22:00 22.5\n#> 120 2023-05-06T23:00 22.5\n#> 121 2023-05-07T00:00 22.6\n#> 122 2023-05-07T01:00 22.5\n#> 123 2023-05-07T02:00 22.4\n#> 124 2023-05-07T03:00 22.3\n#> 125 2023-05-07T04:00 22.1\n#> 126 2023-05-07T05:00 21.9\n#> 127 2023-05-07T06:00 22.1\n#> 128 2023-05-07T07:00 22.8\n#> 129 2023-05-07T08:00 23.8\n#> 130 2023-05-07T09:00 25.4\n#> 131 2023-05-07T10:00 26.6\n#> 132 2023-05-07T11:00 27.9\n#> 133 2023-05-07T12:00 29.2\n#> 134 2023-05-07T13:00 29.6\n#> 135 2023-05-07T14:00 29.7\n#> 136 2023-05-07T15:00 29.4\n#> 137 2023-05-07T16:00   29\n#> 138 2023-05-07T17:00 28.3\n#> 139 2023-05-07T18:00 27.3\n#> 140 2023-05-07T19:00 26.4\n#> 141 2023-05-07T20:00 25.3\n#> 142 2023-05-07T21:00 24.3\n#> 143 2023-05-07T22:00 24.1\n#> 144 2023-05-07T23:00 24.1\n#> 145 2023-05-08T00:00 24.1\n#> 146 2023-05-08T01:00   24\n#> 147 2023-05-08T02:00 23.9\n#> 148 2023-05-08T03:00 23.8\n#> 149 2023-05-08T04:00 23.7\n#> 150 2023-05-08T05:00 23.6\n#> 151 2023-05-08T06:00 23.9\n#> 152 2023-05-08T07:00 24.7\n#> 153 2023-05-08T08:00 25.8\n#> 154 2023-05-08T09:00 27.4\n#> 155 2023-05-08T10:00 28.8\n#> 156 2023-05-08T11:00 30.3\n#> 157 2023-05-08T12:00 31.8\n#> 158 2023-05-08T13:00 32.5\n#> 159 2023-05-08T14:00 32.8\n#> 160 2023-05-08T15:00 32.8\n#> 161 2023-05-08T16:00 32.3\n#> 162 2023-05-08T17:00 31.5\n#> 163 2023-05-08T18:00 30.4\n#> 164 2023-05-08T19:00 29.6\n#> 165 2023-05-08T20:00 28.7\n#> 166 2023-05-08T21:00 27.9\n#> 167 2023-05-08T22:00 27.7\n#> 168 2023-05-08T23:00 27.8\n\nweather_df$time <- as.numeric(as.POSIXct(strptime(weather_df$time, \"%Y-%m-%dT%H:%M\"))) # Convert time to numberic\nweather_df$temp <- as.numeric(weather_df$temp) # Convert temp to numberic\n\nggplot(weather_df, aes(time, temp)) +\n  geom_point() +\n  scale_y_continuous(limits = c(-10, 40)) # Ploting weather\n\n\n\n\n\n\n\n\n# Task 2. Looking at other bike shops.\n\n# Radon-bikes mountainbike category\nurl <- \"https://www.radon-bikes.de/e-bike/mountainbike/\"\n\n# Retrieve names from bikeTitle class\nbike_names <- url %>% \n  read_html() %>% \n  html_nodes(\".bikeTitle > h4\") %>%\n  html_text() \n\n# Retrieve prices from info class\nbike_prices <- url %>% \n  read_html() %>% \n  html_nodes(\".info > div > div > span\") %>%\n  html_text()\n# Since there is a second price in pounds sterling we remove those and only look at the price in euros\nbike_prices <- bike_prices[seq(1,length(bike_prices),2)]\n\n# Transform into tibble and print\ntibble(name = bike_names,price = bike_prices) %>% print()\n\n#> # A tibble: 6 × 2\n#>   name              price\n#>   <chr>             <chr>\n#> 1 RENDER AL         3999 \n#> 2 RENDER            4999 \n#> 3 DEFT              4799 \n#> 4 JEALOUS HYBRID    2499 \n#> 5 ZR TEAM HYBRID CX 1799 \n#> 6 ZR LADY HYBRID CX 2499\n\n# Since there are only 6 different bikes in the mountainbike category, only these are shown"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "First I tried all the example and the business case as can be seen in DATA_WRANGLING.R and DATA_WRANGLING_BUSINESS_CASE.R.\nAfterwards I started with the challenge using the reduced data set due to hardware limitations.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(vroom)\n\n## Data Loading ##\n# Using the reduced data set\n\n# loading patent.tsv file\ncol_types_patent <- list(\n  id = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  num_claims = col_double()\n)\npatent_tbl <- vroom(\n  file       = \"../../src/Patent_data_reduced/patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\npatent_tbl <- rename(patent_tbl,patent_id = id) # Rename id to patent_id to make merging easier\n\nThe loading of the assignee.tsv, patent_assignee.tsv and uspc.tsv files happened similarly to the patent.tsv file\nFor the columns and their types I looked at the contents of the files themselves and the variable types each column contains.\n\n# loading assignee.tsv file\ncol_types_assignee <- list(\n  id = col_character(),\n  type = col_integer(),\n  organization = col_character()\n)\nassignee_tbl <- vroom(\n  file       = \"../../src/Patent_data_reduced/assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_assignee,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\nassignee_tbl <- rename(assignee_tbl,assignee_id = id) # Rename id to assignee_id to make merging easier\n\n# loading patent_assignee.tsv file\ncol_types_patent_assignee <- list(\n  patent_id = col_character(),\n  assignee_id = col_character()\n)\npatent_assignee_tbl <- vroom(\n  file       = \"../../src/Patent_data_reduced/patent_assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent_assignee,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# loading uspc.tsv file\ncol_types_uspc <- list(\n  patent_id = col_character(),\n  mainclass_id = col_character(),\n  sequence = col_integer()\n)\nuspc_tbl <- vroom(\n  file       = \"../../src/Patent_data_reduced/uspc.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_uspc,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n\n1 Visualization\nTo better understand the interactions and combinations of the different tables I constructed a little visualization.\n\n\nTables connections visualization\n\n\n\n2 Patent Dominance\nFor the patent dominance task I performed the following steps:\n\n# Convert patent_assignee_tbl to data.table and group it's entries by assignee_id. Then the number of elements per group is stored in a column patents.\npatent_dominance_dt <- as.data.table(patent_assignee_tbl)[,.(patents = .N),by = assignee_id]\n# Combine patent_activity_dt with assignee_tbl by patent_id to combine assignee info with company-name and -type info.\npatent_dominance_dt <- as.data.table(merge(as_tibble(patent_dominance_dt),assignee_tbl,by=\"assignee_id\")) %>%\n                                    .[type == 2] %>% # Then the data table can be filtered by only allowing US Companies or Corporations\n                                    .[order(-patents),patents,organization] # Afterwards the list is sorted by the number of patents.\n                                                                            # Only the patents and organization name column is kept for better looking output.\nprint(\"What US company / corporation has the most patents?\")\n\n#> [1] \"What US company / corporation has the most patents?\"\n\nprint(patent_dominance_dt[1]) # Take the first element of the ordered list.\n\n#>                                   organization patents\n#> 1: International Business Machines Corporation    7547\n\nprint(\"List the 10 US companies with the most assigned/granted patents.\")\n\n#> [1] \"List the 10 US companies with the most assigned/granted patents.\"\n\nprint(patent_dominance_dt[1:10]) # Take the first 10 elements of the ordered list.\n\n#>                                    organization patents\n#>  1: International Business Machines Corporation    7547\n#>  2:                       Microsoft Corporation    3165\n#>  3:                                 Google Inc.    2668\n#>  4:                       QUALCOMM Incorporated    2597\n#>  5:                                  Apple Inc.    2201\n#>  6:                    General Electric Company    1873\n#>  7:   Hewlett-Packard Development Company, L.P.    1638\n#>  8:          AT&T INTELLECTUAL PROPERTY I, L.P.    1625\n#>  9:                           Intel Corporation    1616\n#> 10:         GM Global Technology Operations LLC    1533\n\n\n\n3 Recent Patent Activity\nSimilarly I performed a few steps (some similar to patent dominance) to find the top companies by patent activity in august of 2014:\n\n# Take patent_tbl and separate into year, month and day and convert to data.table.\npatent_activity_dt <- patent_tbl %>% separate(col = date,into = c(\"year\",\"month\",\"day\"),sep = \"-\",convert = T) %>%\n                                 as.data.table() %>% .[year == 2014 & month == 8] # Then filter by august (8) 2014.\n# Combine patent_activity_dt with patent_assignee_tbl by patent_id to combine patent info with assignee info..\npatent_activity_dt <- as.data.table(merge(as_tibble(patent_activity_dt),patent_assignee_tbl,by=\"patent_id\")) %>%\n                                   .[,.(patents = .N),by = assignee_id] # Group and sum up by assignee_id.\n# Combine patent_activity_dt with assignee_tbl by patent_id to combine assignee info with company-name and -type info.\npatent_activity_dt <- as.data.table(merge(as_tibble(patent_activity_dt),assignee_tbl,by=\"assignee_id\")) %>%\n                                    .[order(-patents),.(organization,type,patents)] # Order by number of patents and discard assignee_id.\n\nprint(\"What US company had the most patents granted in August 2014?\")\n\n#> [1] \"What US company had the most patents granted in August 2014?\"\n\nprint(patent_activity_dt[type == 2][1]) # Filter by US Company or Corporation and take the first element of the ordered list.\n\n#>                                   organization type patents\n#> 1: International Business Machines Corporation    2     718\n\nprint(\"List the top 10 companies with the most new granted patents for August 2014.\")\n\n#> [1] \"List the top 10 companies with the most new granted patents for August 2014.\"\n\nprint(patent_activity_dt[1:10]) # Take the first 10 elements of the ordered list.\n\n#>                                    organization type patents\n#>  1: International Business Machines Corporation    2     718\n#>  2:               Samsung Electronics Co., Ltd.    3     524\n#>  3:                      Canon Kabushiki Kaisha    3     361\n#>  4:                       Microsoft Corporation    2     337\n#>  5:                            Sony Corporation    3     269\n#>  6:                                 Google Inc.    2     240\n#>  7:                       QUALCOMM Incorporated    2     223\n#>  8:                                  Apple Inc.    2     222\n#>  9:                    Kabushiki Kaisha Toshiba    3     213\n#> 10:                         LG Electronics Inc.    3     211\n\n\n\n4 Innovation In Tech\nSimilarly I performed a few steps (some similar to patent dominance and recent patent activity) to find the top mainclasses:\n\n# For most innovative tech sensor group and sum the uspc_tbl by mainclass_id.\nmost_innovative_tech_sectors <- as.data.table(uspc_tbl)[,.(patents = .N),by = mainclass_id] %>%\n                                            .[order(-patents),patents,mainclass_id] # Then order the list by descending patents.\n# Retrieve the assignee_ids of the 10 companies that have the most patents and only keep the assignee_ids of those.\ntop_USPTO_main_classes <- as.data.table(patent_assignee_tbl)[,.(patents = .N),by = assignee_id][1:10][,assignee_id]\n# Search through the patent_assignee_tbl again and only keep those patents that come from one of those 10 companies.\ntop_USPTO_main_classes <- as.data.table(patent_assignee_tbl)[assignee_id %in% top_USPTO_main_classes]\n# With the list of patents combine it by the patent_id with the uspc_tbl.\ntop_USPTO_main_classes <- as.data.table(merge(as_tibble(top_USPTO_main_classes),uspc_tbl,by=\"patent_id\")) %>%\n                                        .[,.(patents = .N),by = mainclass_id] %>% # Group and sum up by mainclass_id.\n                                        .[order(-patents),patents,mainclass_id] # Then order the list by descending patents.\n\nprint(\"What is the most innovative tech sector?\")\n\n#> [1] \"What is the most innovative tech sector?\"\n\nprint(most_innovative_tech_sectors[1]) # Take the first element of the ordered list.\n\n#>    mainclass_id patents\n#> 1:          257   40526\n\nprint(\"For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?\")\n\n#> [1] \"For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?\"\n\nprint(top_USPTO_main_classes[1:10]) # Take the first 10 elements of the ordered list.\n\n#>     mainclass_id patents\n#>  1:          370    1672\n#>  2:          455    1640\n#>  3:          375     743\n#>  4:          257     701\n#>  5:          166     699\n#>  6:          709     417\n#>  7:          244     344\n#>  8:          438     324\n#>  9:          398     323\n#> 10:          507     179"
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "1 Challenge 1\nI created a graph with multiple locations from the beginning of 2020 up until now.\n\nlibrary(data.table)\nlibrary(tidyverse) # loads ggplot2\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(scales)\nlibrary(maps)\n\noptions(repr.plot.width=50, repr.plot.height=3)\n\n# Challenge 1\n\ncovid_data_tbl <- read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\n#> Rows: 306185 Columns: 67\n#> -- Column specification --------------------------------------------------------\n#> Delimiter: \",\"\n#> chr   (4): iso_code, continent, location, tests_units\n#> dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#> date  (1): date\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid_data_graph_tbl <- covid_data_tbl %>% # Read the covid data\n  filter(location == \"Europe\" | location == \"Germany\" | location == \"United Kingdom\" | location == \"France\" | location == \"Spain\" | location == \"United States\") %>% # Filter only the locations specified in the task\n  select(date,total_cases,location) %>% # Only date, total_cases and location are needed\n  filter(!is.na(total_cases)) %>% # Remove those dates where the total_cases number is not a number\n  filter(date < '2022-04-20') # Plot in task stops in may of 2022, so this data stops there as well\n\ncovid_data_dt <- as.data.table(covid_data_graph_tbl) # Convert tibble to data.frame\n\nlast_date_europe <- covid_data_dt[location == \"Europe\"][order(-date)][1]$date\nlast_date_USA <- covid_data_dt[location == \"United States\"][order(-date)][1]$date\n\naddMillions <- function(x, ...) #<== function will add \" %\" to any number, and allows for any additional formatting through \"format\".\n    format(paste0(x/(1e+06), \" M\"), ...)\n\ncovid_data_dt %>% ggplot(aes(x=date,y=total_cases),palette=\"Dark2\") + # plot total_cases over time\n  geom_line(aes(colour=location)) + # each location gets its own line\n  theme(legend.position = \"bottom\") + # position legend at the bottom\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%B '%y\") + # Change the x axis to a date axis with montly intervals and \"month 'year\" labels\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate the x-axis labels by 45 degrees so they don't collide with each other\n  scale_y_continuous(breaks = seq(0, 200000000, by = 50000000), labels = addMillions) + \n  labs( title = \"COVID-19 confirmed cases worldwide\", # Set plot title.\n        subtitle = \"As of 19/04/2022\", # Set plot subtitle.\n        y = \"Cumulative Cases\", # Set plot y-axis label.\n        colour=\"Continent / Country\") + # Set location/country legend title.\n  theme(axis.title.x=element_blank(), # Remove x axis label \n        text = element_text(size=10)) + # Increase text size\n\n  geom_label( # Display geom_label for europe and united states last data point\n    data=covid_data_dt %>% filter((location == \"Europe\" & date == last_date_europe) | (location == \"United States\" & date == last_date_USA)),\n    aes(label=total_cases),hjust=1,vjust=0.4\n  )\n\n\n\n\n\n\n\n\n2 Challenge 2\nI created a world map with a blue color scale showing the relative fatality rate.\n\n# Get Case-Fatality rate (deaths/cases)\ncovid_data_graph_tbl <- covid_data_tbl %>% # Read the covid data.\n  filter(!is.na(total_cases) & !is.na(total_deaths) & !is.na(total_deaths_per_million)) %>% # Remove those dates where the total_cases number is not a number.\n  group_by(location) %>% summarise(total_cases = sum(total_cases),total_deaths = sum(total_deaths),total_deaths_per_million = sum(total_deaths_per_million)) %>% # Group by location (country) and sum up total_cases and total_deaths over all dates.\n  mutate(fatality_rate = (total_deaths/total_cases)) %>% # Add fatality_rate column to the tibble.\n                                                         # Can be exchanged for total_deaths_per_million to visualize mortality rate.\n  select(fatality_rate,location) %>%  # Only maintain fatality_rate and location\n  mutate(location = case_when( # Replace non matching location names\n    \n    location == \"United Kingdom\" ~ \"UK\",\n    location == \"United States\" ~ \"USA\",\n    location == \"Democratic Republic of Congo\" ~ \"Democratic Republic of the Congo\",\n    TRUE ~ location\n    \n  )) %>%\n  distinct()\n\ntotal_deaths_worldwide <- covid_data_tbl %>% filter(!is.na(total_deaths)) %>% group_by(location) %>% filter(row_number()==n()) %>% summarise(total_deaths) %>%\n                                        ungroup() %>% summarise(total_deaths = sum(total_deaths))\n\nlibrary(RColorBrewer)\nlibrary(maptools)\n\n\nworld <- map_data(\"world\")\nggplot(covid_data_graph_tbl) + \n  geom_map(dat=world, map=world, \n           aes(map_id=region), fill=\"white\", color=\"black\") + \n  geom_map(map=world, \n           aes(map_id=location, fill=fatality_rate), color=\"black\") + \n  expand_limits(x = world$long, y = world$lat) +\n  labs( title = \"Confirmed COVID-19 fatality rate.\", # Set plot title.\n        subtitle = paste0(\"Around \",round(total_deaths_worldwide / 1e6, 1),\" Million confirmed COVID-19 deaths worldwide.\"), # Set plot subtitle.\n        caption = paste0(\"Date:\",format(Sys.Date(), format=\"%d/%m/%Y\")), # Set plot caption.\n        fill = \"Fatality rate\") + # Set plot legend caption.\n  theme(axis.title.x=element_blank(), # Remove x axis label. \n        axis.title.y=element_blank(), # Remove y axis label.\n        axis.ticks = element_blank(), # Remove axis ticks.\n        axis.text.x = element_blank(), # Remove x axis texts.\n        axis.text.y = element_blank(), # Remove y axis texts.\n        )"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own."
  }
]