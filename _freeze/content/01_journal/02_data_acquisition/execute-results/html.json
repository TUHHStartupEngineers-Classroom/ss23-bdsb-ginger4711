{
  "hash": "d9b84375a6ee712bb0056b7c10aff984",
  "result": {
    "markdown": "---\ntitle: \"Data Acquisition\"\nauthor: \"Christian Sühl\"\n---\n\n\n\n# Database\nTrying out a connection to a database containing music information using RSQLite.\n\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-2_e66ea890bd0a112074f2f703873c7679'}\n\n```{.r .cell-code}\ncon <- RSQLite::dbConnect(drv    = SQLite(), \n                          dbname = \"../../src/Chinook_Sqlite.sqlite\") # Connect to db\n\nprint(dbListTables(con)) # Print list of available tables\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  [1] \"Album\"         \"Artist\"        \"Customer\"      \"Employee\"     \n#>  [5] \"Genre\"         \"Invoice\"       \"InvoiceLine\"   \"MediaType\"    \n#>  [9] \"Playlist\"      \"PlaylistTrack\" \"Track\"\n```\n:::\n\n```{.r .cell-code}\nalbum_tbl <- tbl(con, \"Album\") %>% collect() # Retrieve \"Album\" table into local storage\nprint(album_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 347 × 3\n#>    AlbumId Title                                 ArtistId\n#>      <int> <chr>                                    <int>\n#>  1       1 For Those About To Rock We Salute You        1\n#>  2       2 Balls to the Wall                            2\n#>  3       3 Restless and Wild                            2\n#>  4       4 Let There Be Rock                            1\n#>  5       5 Big Ones                                     3\n#>  6       6 Jagged Little Pill                           4\n#>  7       7 Facelift                                     5\n#>  8       8 Warner 25 Anos                               6\n#>  9       9 Plays Metallica By Four Cellos               7\n#> 10      10 Audioslave                                   8\n#> # ℹ 337 more rows\n```\n:::\n\n```{.r .cell-code}\ndbDisconnect(con) # Disconnect db session\n```\n:::\n\n\n# API\nTrying out a connection to multiple APIs containing Star Wars information and Stock Prices with credentials.\n\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-4_1a1723a0269ec56ce4f993ab82b9837e'}\n\n```{.r .cell-code}\n# Wrapped into a function\nsw_api <- function(path) {\n  url <- modify_url(url = \"https://swapi.dev\", path = glue(\"/api{path}\"))\n  resp <- GET(url)\n  stop_for_status(resp) # automatically throws an error if a request did not succeed\n}\n\nresp <- sw_api(\"/people/1\") # Retrieve info about Luke Skywalker\n\ncontent <- rawToChar(resp$content) %>% fromJSON() # Turn content into characters and convert json to list\nprint(content)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> $name\n#> [1] \"Luke Skywalker\"\n#> \n#> $height\n#> [1] \"172\"\n#> \n#> $mass\n#> [1] \"77\"\n#> \n#> $hair_color\n#> [1] \"blond\"\n#> \n#> $skin_color\n#> [1] \"fair\"\n#> \n#> $eye_color\n#> [1] \"blue\"\n#> \n#> $birth_year\n#> [1] \"19BBY\"\n#> \n#> $gender\n#> [1] \"male\"\n#> \n#> $homeworld\n#> [1] \"https://swapi.dev/api/planets/1/\"\n#> \n#> $films\n#> [1] \"https://swapi.dev/api/films/1/\" \"https://swapi.dev/api/films/2/\"\n#> [3] \"https://swapi.dev/api/films/3/\" \"https://swapi.dev/api/films/6/\"\n#> \n#> $species\n#> list()\n#> \n#> $vehicles\n#> [1] \"https://swapi.dev/api/vehicles/14/\" \"https://swapi.dev/api/vehicles/30/\"\n#> \n#> $starships\n#> [1] \"https://swapi.dev/api/starships/12/\" \"https://swapi.dev/api/starships/22/\"\n#> \n#> $created\n#> [1] \"2014-12-09T13:50:51.644000Z\"\n#> \n#> $edited\n#> [1] \"2014-12-20T21:17:56.891000Z\"\n#> \n#> $url\n#> [1] \"https://swapi.dev/api/people/1/\"\n```\n:::\n:::\n\n\nNow I created a .Renviron file in the home folder and added the token in there. Now it can be accessed using `Sys.getenv('token')`.\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-5_6a49e6d5dabc231ab45fde758c38fe86'}\n\n```{.r .cell-code}\nalphavantage_api_url <- \"https://www.alphavantage.co/query\"\nticker               <- \"WDI.DE\"\n# You can pass all query parameters as a list to the query argument of GET()\nWDIQuote = GET(alphavantage_api_url, query = list('function' = \"GLOBAL_QUOTE\",\n                                       symbol     = ticker,\n                                       apikey     = Sys.getenv('token'))\n) %>% content()\nprint(WDIQuote)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> $`Global Quote`\n#> $`Global Quote`$`01. symbol`\n#> [1] \"WDI.DE\"\n#> \n#> $`Global Quote`$`02. open`\n#> [1] \"0.4021\"\n#> \n#> $`Global Quote`$`03. high`\n#> [1] \"0.4099\"\n#> \n#> $`Global Quote`$`04. low`\n#> [1] \"0.3961\"\n#> \n#> $`Global Quote`$`05. price`\n#> [1] \"0.3985\"\n#> \n#> $`Global Quote`$`06. volume`\n#> [1] \"210849\"\n#> \n#> $`Global Quote`$`07. latest trading day`\n#> [1] \"2021-03-01\"\n#> \n#> $`Global Quote`$`08. previous close`\n#> [1] \"0.4109\"\n#> \n#> $`Global Quote`$`09. change`\n#> [1] \"-0.0124\"\n#> \n#> $`Global Quote`$`10. change percent`\n#> [1] \"-3.0178%\"\n```\n:::\n:::\n\n\n\n# WEB SCRAPING\n\n\n\nTrying out some webscraping examples from wikipedia and IMDB.\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-7_321676c0f3f756b1663c07328b328d60'}\n\n```{.r .cell-code}\nurl <- \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\nsp_500 <- url %>%\n  # read the HTML from the webpage\n  read_html() %>%\n  # Get the nodes with the id\n  html_nodes(css = \"#constituents\") %>%\n  # html_nodes(xpath = \"//*[@id='constituents']\"\") %>% \n  # Extract the table and turn the list into a tibble\n  html_table() %>% \n  .[[1]] %>% \n  as_tibble()\nprint(sp_500)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 503 x 8\n#>    Symbol Security      `GICS Sector` `GICS Sub-Industry` Headquarters Locatio~1\n#>    <chr>  <chr>         <chr>         <chr>               <chr>                 \n#>  1 MMM    3M            Industrials   Industrial Conglom~ Saint Paul, Minnesota \n#>  2 AOS    A. O. Smith   Industrials   Building Products   Milwaukee, Wisconsin  \n#>  3 ABT    Abbott        Health Care   Health Care Equipm~ North Chicago, Illino~\n#>  4 ABBV   AbbVie        Health Care   Pharmaceuticals     North Chicago, Illino~\n#>  5 ACN    Accenture     Information ~ IT Consulting & Ot~ Dublin, Ireland       \n#>  6 ATVI   Activision B~ Communicatio~ Interactive Home E~ Santa Monica, Califor~\n#>  7 ADM    ADM           Consumer Sta~ Agricultural Produ~ Chicago, Illinois     \n#>  8 ADBE   Adobe Inc.    Information ~ Application Softwa~ San Jose, California  \n#>  9 ADP    ADP           Industrials   Human Resource & E~ Roseland, New Jersey  \n#> 10 AAP    Advance Auto~ Consumer Dis~ Automotive Retail   Raleigh, North Caroli~\n#> # i 493 more rows\n#> # i abbreviated name: 1: `Headquarters Location`\n#> # i 3 more variables: `Date added` <chr>, CIK <int>, Founded <chr>\n```\n:::\n:::\n\nTo try IMDB out I scraped the people that worked on \"The Dark Knight\" that are mentioned in the IMDB top 250 ratings page.\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-8_dbc97de106ed393ce0bcc59175ea4691'}\n\n```{.r .cell-code}\nurl  <- \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\nhtml <- url %>% \n  read_html()\n\npeople <- html %>% \n  html_nodes(\".titleColumn > a\") %>% \n  .[[3]] %>% \n  html_attr(\"title\")\nprint(people)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"Christopher Nolan (dir.), Christian Bale, Heath Ledger\"\n```\n:::\n:::\n\n\n# Functional Programming\nFollowing the example i printed some numbers and retrieved the possible bike colors from the data file.\n\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-10_7a6eca5e9706b9c0f8759707f3fd96e7'}\n\n```{.r .cell-code}\nnumbers <- c(1:3)\nnumbers_list <- map(numbers, print)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 1\n#> [1] 2\n#> [1] 3\n```\n:::\n\n```{.r .cell-code}\nbike_data_lst <- fromJSON(\"../../src/bike_data.json\")\nprint(bike_data_lst %>% purrr::pluck(\"productDetail\", \"variationAttributes\", \"values\", 1, \"displayValue\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"Stealth\"     \"aero silver\"\n```\n:::\n:::\n\n\n# CHALLENGE\n\nAsking the API for the weather on Gran Canaria in this week.\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-11_8f550892946de8c7392f8339ec607909'}\n\n```{.r .cell-code}\n# Task 1. Asking an API.\nlibrary(ggplot2)\n\nweather_api <- function() {\n  url <- modify_url(url = \"https://api.open-meteo.com/v1/forecast?latitude=27.7564556&longitude=-15.5910148&current_weather=true&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\")\n  resp <- GET(url)\n  stop_for_status(resp) # automatically throws an error if a request did not succeed\n}\n\nweather <- weather_api() %>% content()\n\nweather_tbl <- tibble(time = weather[[\"hourly\"]][[\"time\"]],temp = weather[[\"hourly\"]][[\"temperature_2m\"]])\nweather_df <- weather_tbl %>% data.frame\nprint(weather_df) # Print weather as df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                 time temp\n#> 1   2023-04-29T00:00 22.6\n#> 2   2023-04-29T01:00 22.1\n#> 3   2023-04-29T02:00 21.7\n#> 4   2023-04-29T03:00 21.5\n#> 5   2023-04-29T04:00 21.4\n#> 6   2023-04-29T05:00 21.3\n#> 7   2023-04-29T06:00 21.9\n#> 8   2023-04-29T07:00 21.6\n#> 9   2023-04-29T08:00 23.4\n#> 10  2023-04-29T09:00 24.9\n#> 11  2023-04-29T10:00 26.3\n#> 12  2023-04-29T11:00 27.7\n#> 13  2023-04-29T12:00 28.5\n#> 14  2023-04-29T13:00 29.4\n#> 15  2023-04-29T14:00 30.2\n#> 16  2023-04-29T15:00 30.9\n#> 17  2023-04-29T16:00 30.5\n#> 18  2023-04-29T17:00 29.7\n#> 19  2023-04-29T18:00 28.7\n#> 20  2023-04-29T19:00 27.5\n#> 21  2023-04-29T20:00 26.3\n#> 22  2023-04-29T21:00 25.5\n#> 23  2023-04-29T22:00 25.1\n#> 24  2023-04-29T23:00 24.8\n#> 25  2023-04-30T00:00 24.7\n#> 26  2023-04-30T01:00 24.6\n#> 27  2023-04-30T02:00 24.2\n#> 28  2023-04-30T03:00 24.1\n#> 29  2023-04-30T04:00 24.2\n#> 30  2023-04-30T05:00 23.9\n#> 31  2023-04-30T06:00 23.6\n#> 32  2023-04-30T07:00 23.6\n#> 33  2023-04-30T08:00 24.9\n#> 34  2023-04-30T09:00 26.9\n#> 35  2023-04-30T10:00 28.6\n#> 36  2023-04-30T11:00 30.1\n#> 37  2023-04-30T12:00 30.9\n#> 38  2023-04-30T13:00   31\n#> 39  2023-04-30T14:00 31.3\n#> 40  2023-04-30T15:00   31\n#> 41  2023-04-30T16:00   30\n#> 42  2023-04-30T17:00 28.9\n#> 43  2023-04-30T18:00 28.5\n#> 44  2023-04-30T19:00 28.1\n#> 45  2023-04-30T20:00 26.6\n#> 46  2023-04-30T21:00 25.9\n#> 47  2023-04-30T22:00 25.4\n#> 48  2023-04-30T23:00 25.3\n#> 49  2023-05-01T00:00 25.2\n#> 50  2023-05-01T01:00   25\n#> 51  2023-05-01T02:00 24.8\n#> 52  2023-05-01T03:00 24.4\n#> 53  2023-05-01T04:00 23.9\n#> 54  2023-05-01T05:00 23.6\n#> 55  2023-05-01T06:00 23.3\n#> 56  2023-05-01T07:00 23.3\n#> 57  2023-05-01T08:00 24.2\n#> 58  2023-05-01T09:00 25.7\n#> 59  2023-05-01T10:00 27.2\n#> 60  2023-05-01T11:00 28.3\n#> 61  2023-05-01T12:00 29.1\n#> 62  2023-05-01T13:00 29.7\n#> 63  2023-05-01T14:00 30.2\n#> 64  2023-05-01T15:00 30.5\n#> 65  2023-05-01T16:00 30.4\n#> 66  2023-05-01T17:00 29.3\n#> 67  2023-05-01T18:00   29\n#> 68  2023-05-01T19:00 27.8\n#> 69  2023-05-01T20:00   26\n#> 70  2023-05-01T21:00 24.3\n#> 71  2023-05-01T22:00 24.5\n#> 72  2023-05-01T23:00 24.6\n#> 73  2023-05-02T00:00 24.6\n#> 74  2023-05-02T01:00 24.3\n#> 75  2023-05-02T02:00 22.9\n#> 76  2023-05-02T03:00 21.4\n#> 77  2023-05-02T04:00 21.1\n#> 78  2023-05-02T05:00   21\n#> 79  2023-05-02T06:00   21\n#> 80  2023-05-02T07:00 20.7\n#> 81  2023-05-02T08:00 22.7\n#> 82  2023-05-02T09:00 24.4\n#> 83  2023-05-02T10:00 25.1\n#> 84  2023-05-02T11:00 25.8\n#> 85  2023-05-02T12:00 26.9\n#> 86  2023-05-02T13:00 27.4\n#> 87  2023-05-02T14:00 27.4\n#> 88  2023-05-02T15:00 27.8\n#> 89  2023-05-02T16:00 27.9\n#> 90  2023-05-02T17:00 27.5\n#> 91  2023-05-02T18:00   26\n#> 92  2023-05-02T19:00 25.1\n#> 93  2023-05-02T20:00   24\n#> 94  2023-05-02T21:00 22.9\n#> 95  2023-05-02T22:00 22.2\n#> 96  2023-05-02T23:00 21.7\n#> 97  2023-05-03T00:00 21.3\n#> 98  2023-05-03T01:00 21.2\n#> 99  2023-05-03T02:00 20.9\n#> 100 2023-05-03T03:00 20.5\n#> 101 2023-05-03T04:00 20.4\n#> 102 2023-05-03T05:00 20.6\n#> 103 2023-05-03T06:00 20.6\n#> 104 2023-05-03T07:00 21.5\n#> 105 2023-05-03T08:00   23\n#> 106 2023-05-03T09:00 24.8\n#> 107 2023-05-03T10:00 25.8\n#> 108 2023-05-03T11:00 26.7\n#> 109 2023-05-03T12:00 27.4\n#> 110 2023-05-03T13:00 27.4\n#> 111 2023-05-03T14:00 27.2\n#> 112 2023-05-03T15:00 26.8\n#> 113 2023-05-03T16:00 26.5\n#> 114 2023-05-03T17:00 26.2\n#> 115 2023-05-03T18:00 25.6\n#> 116 2023-05-03T19:00 24.9\n#> 117 2023-05-03T20:00 24.1\n#> 118 2023-05-03T21:00 23.2\n#> 119 2023-05-03T22:00 22.9\n#> 120 2023-05-03T23:00 22.7\n#> 121 2023-05-04T00:00 22.4\n#> 122 2023-05-04T01:00 22.2\n#> 123 2023-05-04T02:00 21.9\n#> 124 2023-05-04T03:00 21.6\n#> 125 2023-05-04T04:00 21.3\n#> 126 2023-05-04T05:00 21.1\n#> 127 2023-05-04T06:00 20.8\n#> 128 2023-05-04T07:00   21\n#> 129 2023-05-04T08:00   22\n#> 130 2023-05-04T09:00 23.3\n#> 131 2023-05-04T10:00 24.3\n#> 132 2023-05-04T11:00 25.4\n#> 133 2023-05-04T12:00 26.5\n#> 134 2023-05-04T13:00 26.9\n#> 135 2023-05-04T14:00 27.1\n#> 136 2023-05-04T15:00   27\n#> 137 2023-05-04T16:00 26.6\n#> 138 2023-05-04T17:00 26.1\n#> 139 2023-05-04T18:00 25.1\n#> 140 2023-05-04T19:00 24.2\n#> 141 2023-05-04T20:00 23.2\n#> 142 2023-05-04T21:00   22\n#> 143 2023-05-04T22:00 21.3\n#> 144 2023-05-04T23:00 20.8\n#> 145 2023-05-05T00:00 20.3\n#> 146 2023-05-05T01:00 19.8\n#> 147 2023-05-05T02:00 19.5\n#> 148 2023-05-05T03:00 19.1\n#> 149 2023-05-05T04:00 18.8\n#> 150 2023-05-05T05:00 18.6\n#> 151 2023-05-05T06:00 18.8\n#> 152 2023-05-05T07:00 19.7\n#> 153 2023-05-05T08:00 21.1\n#> 154 2023-05-05T09:00 22.8\n#> 155 2023-05-05T10:00 23.9\n#> 156 2023-05-05T11:00   25\n#> 157 2023-05-05T12:00 26.1\n#> 158 2023-05-05T13:00 26.6\n#> 159 2023-05-05T14:00 26.9\n#> 160 2023-05-05T15:00 26.8\n#> 161 2023-05-05T16:00 26.4\n#> 162 2023-05-05T17:00 25.6\n#> 163 2023-05-05T18:00 24.6\n#> 164 2023-05-05T19:00 23.8\n#> 165 2023-05-05T20:00   23\n#> 166 2023-05-05T21:00 22.1\n#> 167 2023-05-05T22:00 21.7\n#> 168 2023-05-05T23:00 21.5\n```\n:::\n\n```{.r .cell-code}\nweather_df$time <- as.numeric(as.POSIXct(strptime(weather_df$time, \"%Y-%m-%dT%H:%M\"))) # Convert time to numberic\nweather_df$temp <- as.numeric(weather_df$temp) # Convert temp to numberic\n\nggplot(weather_df, aes(time, temp)) +\n  geom_point() +\n  scale_y_continuous(limits = c(-10, 40)) # Ploting weather\n```\n\n::: {.cell-output-display}\n![](02_data_acquisition_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-12_d1b0ccd4374873daf9a9f295ab047510'}\n\n```{.r .cell-code}\n# Task 2. Looking at other bike shops.\n\n# Radon-bikes mountainbike category\nurl <- \"https://www.radon-bikes.de/e-bike/mountainbike/\"\n\n# Retrieve names from bikeTitle class\nbike_names <- url %>% \n  read_html() %>% \n  html_nodes(\".bikeTitle > h4\") %>%\n  html_text() \n\n# Retrieve prices from info class\nbike_prices <- url %>% \n  read_html() %>% \n  html_nodes(\".info > div > div > span\") %>%\n  html_text()\n# Since there is a second price in pounds sterling we remove those and only look at the price in euros\nbike_prices <- bike_prices[seq(1,length(bike_prices),2)]\n\n# Transform into tibble and print\ntibble(name = bike_names,price = bike_prices) %>% print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 6 x 2\n#>   name              price\n#>   <chr>             <chr>\n#> 1 RENDER AL         3999 \n#> 2 RENDER            4999 \n#> 3 DEFT              4799 \n#> 4 JEALOUS HYBRID    2499 \n#> 5 ZR TEAM HYBRID CX 1799 \n#> 6 ZR LADY HYBRID CX 2499\n```\n:::\n\n```{.r .cell-code}\n# Since there are only 6 different bikes in the mountainbike category, only these are shown\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}